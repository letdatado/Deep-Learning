{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uq2mBOGkbXa7",
        "outputId": "339c7cc3-b47b-41ed-ad6d-3e21be8c804a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPzpweBoaKZr"
      },
      "source": [
        "# Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install face_recognition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQ4o3gVmb3HF",
        "outputId": "dbabf241-68b2-48fa-cfe7-0d5fd63df438"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting face_recognition\n",
            "  Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from face_recognition) (1.21.6)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (19.18.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from face_recognition) (7.1.2)\n",
            "Collecting face-recognition-models>=0.3.0\n",
            "  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 100.1 MB 26 kB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (7.1.2)\n",
            "Building wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566186 sha256=b45da7feb672471328f355ff618d6d1905e550b6083aab7b532fab777b57d04e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/81/3c/884bcd5e1c120ff548d57c2ecc9ebf3281c9a6f7c0e7e7947a\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face-recognition\n",
            "Successfully installed face-recognition-1.3.0 face-recognition-models-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ci14HGXKaKZv"
      },
      "outputs": [],
      "source": [
        "import time, os, sys, pickle, face_recognition, cv2, sklearn\n",
        "from google.colab.patches import cv2_imshow\n",
        "from sklearn import svm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWSVq4zjaKZ0"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "VdQLhpQaaKZ1",
        "outputId": "fa04c35f-8c0a-49cf-8a6b-6e29b64d0c8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trump/donald trump98.jpg was skipped and can't be used for training\n",
            "Trump/donald trump speech129.jpg was skipped and can't be used for training\n",
            "Trump/trump37.jpg was skipped and can't be used for training\n",
            "Trump/donald trump speech131.jpg was skipped and can't be used for training\n",
            "Trump/donald trump68.jpg was skipped and can't be used for training\n",
            "Trump/donald trump85.jpg was skipped and can't be used for training\n",
            "Trump/donald trump100.jpg was skipped and can't be used for training\n",
            "Trump/donald trump speech123.jpg was skipped and can't be used for training\n",
            "Trump/donald trump speech133.jpg was skipped and can't be used for training\n",
            "Trump/trump50.jpg was skipped and can't be used for training\n",
            "Bill/gates141.jpg was skipped and can't be used for training\n",
            "Bill/gates32.jpg was skipped and can't be used for training\n",
            "Bill/gates33.jpg was skipped and can't be used for training\n",
            "Bill/gates17.jpg was skipped and can't be used for training\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-1b41c42c5b17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Get the face encodings for the face in each image file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_image_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mperson\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mperson_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mface_bounding_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#If training image contains exactly one face\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/face_recognition/api.py\u001b[0m in \u001b[0;36mface_locations\u001b[0;34m(img, number_of_times_to_upsample, model)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_trim_css_to_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_rect_to_css\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mface\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_raw_face_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cnn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_trim_css_to_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_rect_to_css\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mface\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_raw_face_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/face_recognition/api.py\u001b[0m in \u001b[0;36m_raw_face_locations\u001b[0;34m(img, number_of_times_to_upsample, model)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcnn_face_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mface_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# Training the SVC classifier\n",
        "\n",
        "# The training data would be all the face encodings from all the known images and the labels are their names\n",
        "encodings = []\n",
        "names = []\n",
        "persons_found = []\n",
        "\n",
        "# Training directory, here you enter the path of train directory\n",
        "path = '/content/drive/MyDrive/noman_image_recognizer/train_imgs/' #Put here the path of dir that contains training data\n",
        "\n",
        "train_dir = os.listdir(path) \n",
        "\n",
        "# Loop through each person in the training directory\n",
        "for person in train_dir:\n",
        "    pix = os.listdir(path + person)\n",
        "\n",
        "    # Loop through each training image for the current person\n",
        "    for person_img in pix:\n",
        "        # Get the face encodings for the face in each image file\n",
        "        face = face_recognition.load_image_file(path + person + \"/\" + person_img)\n",
        "        face_bounding_boxes = face_recognition.face_locations(face)\n",
        "\n",
        "        #If training image contains exactly one face\n",
        "        if len(face_bounding_boxes) == 1:\n",
        "            face_enc = face_recognition.face_encodings(face)[0]\n",
        "            # Add face encoding for current image\n",
        "            encodings.append(face_enc)\n",
        "            # Add corresponding label\n",
        "            names.append(person)\n",
        "        else:\n",
        "            print(person + \"/\" + person_img + \" was skipped and can't be used for training\")\n",
        "\n",
        "# Create and train the SVC classifier\n",
        "clf = svm.SVC(gamma='scale')\n",
        "\n",
        "# fit the model\n",
        "clf.fit(encodings,names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jzH9YRg9aKZ2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "0af96138-6111-4cfc-8ec6-7724e4db9b94"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-4cb1a4d3fc2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#storing trained SVM Classifier into a file for future usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/noman_image_recognizer/face_classifier_upd2.model'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#creat a file for storing model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#storing model in newly-creasted file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"
          ]
        }
      ],
      "source": [
        "#storing trained SVM Classifier into a file for future usage\n",
        "file = open('/content/drive/MyDrive/noman_image_recognizer/face_classifier_upd2.model','wb') #creat a file for storing model\n",
        "pickle.dump(clf,file) #storing model in newly-creasted file\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p43zK-jBaKZ3"
      },
      "source": [
        "# Face Recognition on Images\n",
        "\n",
        "This Face Recognition model is trained on following 8 Classes.\n",
        "\n",
        "Imran Khan\n",
        "\n",
        "Bill Gates\n",
        "\n",
        "Elon Musk\n",
        "\n",
        "Jeff Bezos\n",
        "\n",
        "Jack Ma\n",
        "\n",
        "Narendra Modi\n",
        "\n",
        "Donald Trump\n",
        "\n",
        "Leonardo Decaprio \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "jGXv2eGBaKZ3",
        "outputId": "9d90ebf5-133c-4e9a-adfa-33788940a683"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-a29d1e2ee618>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Loading saved model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/noman_image_recognizer/noman_image_recognizer/face_classifier_upd2.model'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Test directory path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/noman_image_recognizer/noman_image_recognizer/face_classifier_upd2.model'"
          ]
        }
      ],
      "source": [
        "\n",
        "#Loading saved model\n",
        "clf = pickle.load(open('/content/drive/MyDrive/noman_image_recognizer/face_classifier_upd2.model','rb'))\n",
        "\n",
        "# Test directory path\n",
        "test_dir_path = '/content/drive/MyDrive/noman_image_recognizer/test_imgs/'\n",
        "\n",
        "# Path where to save the recognized images\n",
        "save_dir = '/content/drive/MyDrive/noman_image_recognizer/recognized_imgs/'\n",
        "\n",
        "# if path doesn't exist then make one\n",
        "if not os.path.exists(save_dir): \n",
        "    os.mkdir(save_dir)\n",
        "\n",
        "for img in os.listdir(test_dir_path):\n",
        "    tick = time.time()\n",
        "    # Load the test image with unknown faces into a numpy array\n",
        "    test_image = face_recognition.load_image_file(test_dir_path+img)\n",
        "    # convert an image from one color space to another\n",
        "    test_image = cv2.cvtColor(test_image,cv2.COLOR_BGR2RGB)\n",
        "    # print((test_image.shape))\n",
        "    # Find all the faces in the test image using the default HOG-based model\n",
        "    face_locations = face_recognition.face_locations(test_image)\n",
        "    \n",
        "    no = len(face_locations)\n",
        "    print(\"Number of faces detected: \", no)\n",
        "\n",
        "    # Predict all the faces in the test image using the trained classifier\n",
        "    print(\"Found:\")\n",
        "    for i in range(no):\n",
        "        test_image_enc = face_recognition.face_encodings(test_image,face_locations)[i]\n",
        "        name = clf.predict([test_image_enc])\n",
        "        print((*name))\n",
        "        \n",
        "        top,right,bottom,left = face_locations[i]\n",
        "        \n",
        "        \n",
        "        # Draw a box around the face\n",
        "        cv2.rectangle(test_image, (left, top), (right, bottom), (0, 0, 255), 2)\n",
        "\n",
        "        # Draw a label with a name below the face\n",
        "        cv2.rectangle(test_image, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
        "        font = cv2.FONT_HERSHEY_DUPLEX\n",
        "        cv2.putText(test_image, *name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
        "    cv2.imwrite(save_dir+img,test_image)\n",
        "        \n",
        "        \n",
        "    tock = time.time()\n",
        "    print('time taken: ',tock-tick,' secs')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdZnC47gaKZ4"
      },
      "source": [
        "# Face Recognition on Video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMWVbqq2aKZ4"
      },
      "source": [
        "## Video File Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaYAjf76aKZ4"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "model = pickle.load(open('/content/drive/MyDrive/noman_image_recognizer/face_classifier_upd2.model','rb')) #model file's path\n",
        "\n",
        "# Path to the video file\n",
        "video_file = '/content/drive/MyDrive/noman_image_recognizer/sample.mp4'  #Path to video file\n",
        "\n",
        "\n",
        "# Instantiate the video file\n",
        "vid = cv2.VideoCapture(video_file)\n",
        "frame_width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = vid.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "tick = time.time()\n",
        "output = cv2.VideoWriter(f'output_for_{video_file}.mp4', cv2.VideoWriter_fourcc(*'MP4V'),fps,(frame_width,frame_height))\n",
        "\n",
        "\n",
        "# Read the video frame by frame\n",
        "while (vid.isOpened()):\n",
        "    ret,frame = vid.read()\n",
        "    \n",
        "\n",
        "    if ret:\n",
        "\n",
        "        small_frame = cv2.resize(frame, (0, 0), fx=0.50, fy=0.50)\n",
        "        # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
        "        rgb_small_frame = cv2.cvtColor(small_frame,cv2.COLOR_BGR2RGB)\n",
        "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
        "        num_of_faces = len(face_locations)\n",
        "        for i in range(num_of_faces):\n",
        "\n",
        "            \n",
        "            face_enc = face_recognition.face_encodings(rgb_small_frame)[i]\n",
        "            name = model.predict([face_enc])\n",
        "            \n",
        "            top,right,bottom,left = face_locations[i]\n",
        "            top *= 2\n",
        "            right *= 2\n",
        "            bottom *= 2\n",
        "            left *= 2\n",
        "\n",
        "            # Draw a box around the face\n",
        "            cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
        "\n",
        "            # Draw a label with a name below the face\n",
        "            cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
        "            font = cv2.FONT_HERSHEY_DUPLEX\n",
        "            cv2.putText(frame, *name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
        "            tock = time.time()\n",
        "\n",
        "        # Display the resulting image\n",
        "        output.write(frame)\n",
        "     #   cv2.imshow('video',frame)\n",
        "\n",
        "        # Hit 'q' on the keyboard to quit!\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "    else:break\n",
        "tock = time.time()\n",
        "\n",
        "vid.release()\n",
        "output.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "print('time taken to process video: ',tock-tick)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L088HozHaKZ5"
      },
      "source": [
        "## Webcam Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6oomggDaKZ5"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = pickle.load(open('/content/drive/MyDrive/noman_image_recognizer/face_classifier_upd2.model','rb')) #Path to model file\n",
        "vid = cv2.VideoCapture(0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "while (vid.isOpened()):\n",
        "    ret,frame = vid.read()\n",
        "    \n",
        "\n",
        "    if ret:\n",
        "        small_frame = cv2.resize(frame, (0, 0), fx=0.2, fy=0.20)\n",
        "        # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
        "        rgb_small_frame = cv2.cvtColor(small_frame,cv2.COLOR_BGR2RGB)\n",
        "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
        "        num_of_faces = len(face_locations)\n",
        "        for i in range(num_of_faces):            \n",
        "            face_enc = face_recognition.face_encodings(rgb_small_frame,face_locations)[i]\n",
        "            name = model.predict([face_enc])\n",
        "            \n",
        "            top,right,bottom,left = face_locations[i]\n",
        "            top *= 5\n",
        "            right *= 5\n",
        "            bottom *= 5\n",
        "            left *= 5\n",
        "\n",
        "            # Draw a box around the face\n",
        "            cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
        "\n",
        "            # Draw a label with a name below the face\n",
        "            cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
        "            font = cv2.FONT_HERSHEY_DUPLEX\n",
        "            cv2.putText(frame, *name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
        "            tock = time.time()\n",
        "#             print(f'took {tock-tick} time!!!')\n",
        "#             sys.exit()\n",
        "        # Display the resulting image\n",
        "        cv2.imshow('Cam_Feed',frame)\n",
        "\n",
        "        # Hit 'q' on the keyboard to quit!\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "    else:break\n",
        "\n",
        "vid.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "an-vQ0M4aKZ5"
      },
      "outputs": [],
      "source": [
        "vidcap = cv2.VideoCapture('noman_vid.mp4')\n",
        "success,image = vidcap.read()\n",
        "count = 0\n",
        "while success:\n",
        "  cv2.imwrite(\"frame%d.jpg\" % count, image)     # save frame as JPEG file      \n",
        "  success,image = vidcap.read()\n",
        "  print('Read a new frame: ', success)\n",
        "  count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VI-yNDuqaKZ6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpSTZqVYaKZ6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Copy of Face_Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}