{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7efd316b",
   "metadata": {},
   "source": [
    "# Recognition of the Speaker\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5edb1503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6454a78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "DATASET = os.path.join(os.path.expanduser(\"~\"), \"Speaker Recognition\\\\16000_pcm_speeches\")\n",
    "\n",
    "# Folders for audio and noise samples\n",
    "AUDIO = \"audio\"\n",
    "NOISE = \"noise\"\n",
    "\n",
    "AUDIO_PATH = os.path.join(DATASET, AUDIO)\n",
    "NOISE_PATH = os.path.join(DATASET, NOISE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8c2330",
   "metadata": {},
   "source": [
    "## Setting up Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79892b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 files belonging to 2 directories\n"
     ]
    }
   ],
   "source": [
    "# If folders `audio` and `noise` do not exist, create it, otherwise do nothing\n",
    "if os.path.exists(AUDIO_PATH) is False:\n",
    "    # make new directory\n",
    "    os.makedirs(AUDIO_PATH)\n",
    "if os.path.exists(NOISE_PATH) is False:\n",
    "    # make new directory\n",
    "    os.makedirs(NOISE_PATH)\n",
    "\n",
    "\n",
    "\n",
    "# If folder is `\n",
    "for folder in os.listdir(DATASET):\n",
    "    if os.path.isdir(os.path.join(DATASET, folder)):\n",
    "        if folder in [AUDIO, NOISE]:\n",
    "            # If folder is `audio` or `noise`, do nothing\n",
    "            continue\n",
    "        elif folder in [\"other\", \"_background_noise_\"]:\n",
    "            # else move it to the `noise` folder\n",
    "            shutil.move(\n",
    "                os.path.join(DATASET, folder),\n",
    "                os.path.join(NOISE_PATH, folder),\n",
    "            )\n",
    "        else:\n",
    "            # Otherwise, it should be a speaker folder, then move it to `audio` folder\n",
    "            shutil.move(\n",
    "                os.path.join(DATASET, folder),\n",
    "                os.path.join(AUDIO_PATH, folder),\n",
    "            )\n",
    "\n",
    "\n",
    "# Get the list of all noise files\n",
    "noise_paths = []\n",
    "for subdir in os.listdir(NOISE_PATH):\n",
    "    subdir_path = Path(NOISE_PATH) / subdir\n",
    "    if os.path.isdir(subdir_path):\n",
    "        noise_paths += [\n",
    "            os.path.join(subdir_path, filepath)\n",
    "            for filepath in os.listdir(subdir_path)\n",
    "            if filepath.endswith(\".wav\")\n",
    "        ]\n",
    "\n",
    "print(\n",
    "    \"Found {} files belonging to {} directories\".format(\n",
    "        len(noise_paths), len(os.listdir(NOISE_PATH))\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa4f09c",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe19c175",
   "metadata": {},
   "source": [
    "Setting up configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e5039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take 10% samples for validation purposes\n",
    "SPLIT = 0.1\n",
    "# To shuffle the noise and samples\n",
    "SEED = 34\n",
    "# The sampling rate for all the audio samples\n",
    "SAMPLING_RATE = 16000\n",
    "# The factor to multiply noises \n",
    "SCALE = 0.5\n",
    "# Batch size per epoch\n",
    "BATCH_SIZE = 128\n",
    "# Number of epochs\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d910253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split noise into chunks of 16,000 steps each\n",
    "def load_noise_sample(path):\n",
    "    sample, sampling_rate = tf.audio.decode_wav(\n",
    "        tf.io.read_file(path), desired_channels=1\n",
    "    )\n",
    "    if sampling_rate == SAMPLING_RATE: # remember, we set the sampling rate to be 16000\n",
    "        slices = int(sample.shape[0] / SAMPLING_RATE)\n",
    "        sample = tf.split(sample[: slices * SAMPLING_RATE], slices)\n",
    "        return sample\n",
    "    else:\n",
    "        print(\"Sampling rate for {} is incorrect. Ignoring it\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8d42fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the noises\n",
    "noises = []\n",
    "for path in noise_paths:\n",
    "    sample = load_noise_sample(path)\n",
    "    if sample:\n",
    "        noises.extend(sample)\n",
    "noises = tf.stack(noises)\n",
    "\n",
    "print(\"{} noise files were split into {} noise samples where each is {} sec. long\".format(\n",
    "    len(noise_paths), noises.shape[0], noises.shape[1] // SAMPLING_RATE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adce286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructs the dataset of audio and labels\n",
    "def paths_and_labels_to_dataset(audio_paths, labels):\n",
    "    # path\n",
    "    path = tf.data.Dataset.from_tensor_slices(audio_paths)\n",
    "    # audio \n",
    "    audio = path.map(lambda x: path_to_audio(x))\n",
    "    # labels\n",
    "    label = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    return tf.data.Dataset.zip((audio, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea605564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decodes the audio file\n",
    "def path_to_audio(path):\n",
    "    audio = tf.io.read_file(path)\n",
    "    audio, _ = tf.audio.decode_wav(audio, 1, SAMPLING_RATE)\n",
    "    return audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fde86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds noise \n",
    "def add_noise(audio, noises=None, scale=0.5):\n",
    "    if noises is not None:\n",
    "        # Create a random tensor of the same size as audio ranging from\n",
    "        # 0 to the number of noise stream samples that we have.\n",
    "        tf_rnd = tf.random.uniform(\n",
    "            (tf.shape(audio)[0],), 0, noises.shape[0], dtype=tf.int32\n",
    "        )\n",
    "        noise = tf.gather(noises, tf_rnd, axis=0)\n",
    "        \n",
    "        # Get the amplitude proportion between the audio and the noise\n",
    "        prop = tf.math.reduce_max(audio, axis=1) / tf.math.reduce_max(noise, axis=1)\n",
    "        prop = tf.repeat(tf.expand_dims(prop, axis=1), tf.shape(audio)[1], axis=1)\n",
    "\n",
    "        # Adding the rescaled noise to audio\n",
    "        audio = audio + noise * prop * scale\n",
    "\n",
    "    return audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1840e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourier Transformation\n",
    "def audio_ff_transformation(audio):\n",
    "    # Since tf.signal.fft applies FFT on the innermost dimension,\n",
    "    # we need to squeeze the dimensions and then expand them again\n",
    "    # after FFT\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    fft = tf.signal.fft(\n",
    "        tf.cast(tf.complex(real=audio, imag=tf.zeros_like(audio)), tf.complex64)\n",
    "    )\n",
    "    fft = tf.expand_dims(fft, axis=-1)\n",
    "\n",
    "    # Return the absolute value of the first half of the FFT\n",
    "    # which represents the positive frequencies\n",
    "    return tf.math.abs(fft[:, : (audio.shape[1] // 2), :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb79567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of audio file paths along with their corresponding labels\n",
    "\n",
    "class_names = os.listdir(AUDIO_PATH)\n",
    "print(\"Our class names: {}\".format(class_names,))\n",
    "\n",
    "audio_paths = []\n",
    "labels = []\n",
    "for label, name in enumerate(class_names):\n",
    "    print(\"Processing speaker {}\".format(name,))\n",
    "    dir_path = Path(AUDIO_PATH) / name\n",
    "    speaker_sample_paths = [\n",
    "        os.path.join(dir_path, filepath)\n",
    "        for filepath in os.listdir(dir_path)\n",
    "        if filepath.endswith(\".wav\")\n",
    "    ]\n",
    "    audio_paths += speaker_sample_paths\n",
    "    labels += [label] * len(speaker_sample_paths)\n",
    "\n",
    "print(\n",
    "    \"Found {} files belonging to {} classes.\".format(len(audio_paths), len(class_names))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e26ffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "random_gen = np.random.RandomState(SEED)\n",
    "random_gen.shuffle(audio_paths)\n",
    "random_gen = np.random.RandomState(SEED)\n",
    "random_gen.shuffle(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72be65c6",
   "metadata": {},
   "source": [
    "## Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c10f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and validation\n",
    "num_val_samples = int(SPLIT * len(audio_paths))\n",
    "print(\"Using {} files for training.\".format(len(audio_paths) - num_val_samples))\n",
    "train_audio_paths = audio_paths[:-num_val_samples]\n",
    "train_labels = labels[:-num_val_samples]\n",
    "\n",
    "print(\"Using {} files for validation.\".format(num_val_samples))\n",
    "valid_audio_paths = audio_paths[-num_val_samples:]\n",
    "valid_labels = labels[-num_val_samples:]\n",
    "\n",
    "# Create 2 datasets, one for training and the other for validation\n",
    "train_set = paths_and_labels_to_dataset(train_audio_paths, train_labels)\n",
    "train_set = train_set.shuffle(buffer_size=BATCH_SIZE * 8, seed=SEED).batch(\n",
    "    BATCH_SIZE\n",
    ")\n",
    "\n",
    "valid_set = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n",
    "valid_set = valid_set.shuffle(buffer_size=32 * 8, seed=SEED).batch(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e82504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding noise to train data\n",
    "train_set = train_set.map(\n",
    "    lambda x, y: (add_noise(x, noises, scale=SCALE), y),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")\n",
    "\n",
    "# Transform audio wave to the frequency domain using audio_ff_transformation (Fast Fourier Transformation)\n",
    "train_set = train_set.map(\n",
    "    lambda x, y: (audio_ff_transformation(x), y), num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "train_set = train_set.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "valid_set = valid_set.map(\n",
    "    lambda x, y: (audio_ff_transformation(x), y), num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "valid_set = valid_set.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2b7157",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03add9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_block(x, filters, conv_num=3, activation=\"relu\"):\n",
    "    # Developing Resnet blocks\n",
    "    s = keras.layers.Conv1D(filters, 1, padding=\"same\")(x)\n",
    "    for i in range(conv_num - 1): # 2\n",
    "        x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n",
    "        x = keras.layers.Activation(activation)(x)\n",
    "    x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n",
    "    x = keras.layers.Add()([x, s])\n",
    "    x = keras.layers.Activation(activation)(x)\n",
    "    return keras.layers.MaxPool1D(pool_size=2, strides=2)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866c7c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, num_classes):\n",
    "    inputs = keras.layers.Input(shape=input_shape, name=\"input\")\n",
    "    # Deploying Resnet blocks\n",
    "    x = resnet_block(inputs, 16, 2)\n",
    "    x = resnet_block(x, 32, 2)\n",
    "    x = resnet_block(x, 64, 3)\n",
    "    # pooling layer\n",
    "    x = keras.layers.AveragePooling1D(pool_size=3, strides=3)(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    # two dense layers with RELU\n",
    "    x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "    # Output with Softmax activation\n",
    "    outputs = keras.layers.Dense(num_classes, activation=\"softmax\", name=\"output\")(x)\n",
    "\n",
    "    return keras.models.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddd6286",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model((SAMPLING_RATE // 2, 1), len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8f6af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "\n",
    "# Compile the model using Adam's default learning rate\n",
    "model.compile(\n",
    "    optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69504f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_filename = \"model.h5\"\n",
    "\n",
    "\n",
    "# Early Stopping to stop training if model doesn't improve\n",
    "earlystopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "# Model checkpoint to make sure that model has the best validation accuracy\n",
    "mdlcheckpoint_cb = keras.callbacks.ModelCheckpoint(model_save_filename, monitor=\"val_accuracy\", save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15ee804",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b360efd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "history = model.fit(\n",
    "    train_set,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    validation_data=valid_set,\n",
    "    callbacks=[earlystopping_cb, mdlcheckpoint_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac4b2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "print(model.evaluate(valid_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bab8cd",
   "metadata": {},
   "source": [
    "## Testing the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ad132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES_TO_DISPLAY = 10\n",
    "\n",
    "test_set = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n",
    "test_set = test_set.shuffle(buffer_size=BATCH_SIZE * 8, seed=SEED).batch(\n",
    "    BATCH_SIZE\n",
    ")\n",
    "\n",
    "test_set = test_set.map(lambda x, y: (add_noise(x, noises, scale=SCALE), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6638ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for audios, labels in test_set.take(1):\n",
    "    # Get the signal FFT\n",
    "    ffts = audio_ff_transformation(audios)\n",
    "    # Predict\n",
    "    y_pred = model.predict(ffts)\n",
    "    # Take random samples\n",
    "    rnd = np.random.randint(0, BATCH_SIZE, SAMPLES_TO_DISPLAY)\n",
    "    # adjusting back the audios and labels\n",
    "    audios, labels = audios.numpy()[rnd, :, :], labels.numpy()[rnd]\n",
    "    y_pred = np.argmax(y_pred, axis=-1)[rnd]\n",
    "\n",
    "    for index in range(SAMPLES_TO_DISPLAY):\n",
    "        # For every sample, print the true and predicted label\n",
    "        # as well as run the voice with the noise\n",
    "        print(\n",
    "            \"Speaker:\\33{} {}\\33[0m\\tPredicted:\\33{} {}\\33[0m\".format(\n",
    "                \"[92m\" if labels[index] == y_pred[index] else \"[91m\",\n",
    "                class_names[labels[index]],\n",
    "                \"[92m\" if labels[index] == y_pred[index] else \"[91m\",\n",
    "                class_names[y_pred[index]],\n",
    "            )\n",
    "        )\n",
    "        display(Audio(audios[index, :, :].squeeze(), rate=SAMPLING_RATE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
