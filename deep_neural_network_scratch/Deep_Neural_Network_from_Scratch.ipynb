{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_Neural_Network_from_Scratch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Impoting Libraries and Dataset"
      ],
      "metadata": {
        "id": "8B5tG_Eq-aT2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5keVtEst7yPp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "((X, y), (X_test, y_test)) = fashion_mnist.load_data()\n",
        "\n",
        "# Shuffle the training dataset\n",
        "keys = np.array(range(X.shape[0]))\n",
        "np.random.shuffle(keys)\n",
        "X = X[keys]\n",
        "y = y[keys]\n",
        "\n",
        "# Scale and reshape samples\n",
        "X = (X.reshape(X.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) -\n",
        "             127.5) / 127.5\n"
      ],
      "metadata": {
        "id": "A1HmlcDi-jAs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "994142a4-d019-40ee-e3fc-a1e635103a8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sigmoid and ReLU Layers with forward and backward propagations."
      ],
      "metadata": {
        "id": "dpYR0DQy70tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLU activation\n",
        "class Activation_ReLU:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, training):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Since we need to modify original variable,\n",
        "        # let's make a copy of values first\n",
        "        self.dinputs = dvalues.copy()\n",
        "\n",
        "        # Zero gradient where input values were negative\n",
        "        self.dinputs[self.inputs <= 0] = 0\n",
        "\n",
        "    # Calculate predictions for outputs\n",
        "    def predictions(self, outputs):\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "aq-0nigM8hxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid activation\n",
        "class Activation_Sigmoid:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, training):\n",
        "        # Save input and calculate/save output\n",
        "        # of the sigmoid function\n",
        "        self.inputs = inputs\n",
        "        self.output = 1 / (1 + np.exp(-inputs))\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Derivative - calculates from output of the sigmoid function\n",
        "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
        "\n",
        "    # Calculate predictions for outputs\n",
        "    def predictions(self, outputs):\n",
        "        return (outputs > 0.5) * 1\n"
      ],
      "metadata": {
        "id": "1Um2qlFB8iU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Softmax Output Layer"
      ],
      "metadata": {
        "id": "2_S-Jqab71H_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax activation\n",
        "class Activation_Softmax:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, training):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "\n",
        "        # Get unnormalized probabilities\n",
        "        exp_vals= np.exp(inputs - np.max(inputs, axis=1,keepdims=True))\n",
        "\n",
        "        # Normalize them for each sample\n",
        "        probabilities = exp_vals / np.sum(exp_vals, axis=1,\n",
        "                                            keepdims=True)\n",
        "\n",
        "        self.output = probabilities\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "\n",
        "        # Create uninitialized array\n",
        "        self.dinputs = np.empty_like(dvalues)\n",
        "\n",
        "        # Enumerate outputs and gradients\n",
        "        for index, (single_output, single_dvalues) in \\\n",
        "                enumerate(zip(self.output, dvalues)):\n",
        "            # Flatten output array\n",
        "            single_output = single_output.reshape(-1, 1)\n",
        "            # Calculate Jacobian matrix of the output\n",
        "            jacobian_matrix = np.diagflat(single_output) - \\\n",
        "                              np.dot(single_output, single_output.T)\n",
        "            # Calculate sample-wise gradient\n",
        "            # and add it to the array of sample gradients\n",
        "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
        "                                         single_dvalues)\n",
        "\n",
        "    # Calculate predictions for outputs\n",
        "    def predictions(self, outputs):\n",
        "        return np.argmax(outputs, axis=1)\n"
      ],
      "metadata": {
        "id": "-RpJH46E88J-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Towards a fully parameterized neural network"
      ],
      "metadata": {
        "id": "T_oWlWcf8QHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense layer\n",
        "class Layer_Dense:\n",
        "\n",
        "    # Layer initialization\n",
        "    def __init__(self, n_inputs, n_neurons,\n",
        "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
        "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
        "        # Initialize weights and biases\n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "        # Set regularization strength\n",
        "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
        "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
        "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
        "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, training):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs, weights and biases\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradients on parameters\n",
        "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "\n",
        "\n",
        "        # Gradients on regularization\n",
        "        # L1 on weights\n",
        "        if self.weight_regularizer_l1 > 0:\n",
        "            dL1 = np.ones_like(self.weights)\n",
        "            dL1[self.weights < 0] = -1\n",
        "            self.dweights += self.weight_regularizer_l1 * dL1\n",
        "        # L2 on weights\n",
        "        if self.weight_regularizer_l2 > 0:\n",
        "            self.dweights += 2 * self.weight_regularizer_l2 * \\\n",
        "                             self.weights\n",
        "        # L1 on biases\n",
        "        if self.bias_regularizer_l1 > 0:\n",
        "            dL1 = np.ones_like(self.biases)\n",
        "            dL1[self.biases < 0] = -1\n",
        "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
        "        # L2 on biases\n",
        "        if self.bias_regularizer_l2 > 0:\n",
        "            self.dbiases += 2 * self.bias_regularizer_l2 * \\\n",
        "                            self.biases\n",
        "\n",
        "        # Gradient on values\n",
        "        self.dinputs = np.dot(dvalues, self.weights.T)\n"
      ],
      "metadata": {
        "id": "Cw69fjdb9Jfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropout\n",
        "class Layer_Dropout:\n",
        "\n",
        "    # Init\n",
        "    def __init__(self, rate):\n",
        "        # Store rate, we invert it as for example for dropout\n",
        "        # of 0.1 we need success rate of 0.9\n",
        "        self.rate = 1 - rate\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, training):\n",
        "        # Save input values\n",
        "        self.inputs = inputs\n",
        "\n",
        "        # If not in the training mode - return values\n",
        "        if not training:\n",
        "            self.output = inputs.copy()\n",
        "            return\n",
        "\n",
        "        # Generate and save scaled mask\n",
        "        self.binary_mask = np.random.binomial(1, self.rate,\n",
        "                           size=inputs.shape) / self.rate\n",
        "        # Apply mask to output values\n",
        "        self.output = inputs * self.binary_mask\n",
        "\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradient on values\n",
        "        self.dinputs = dvalues * self.binary_mask"
      ],
      "metadata": {
        "id": "mSmIx92K9QG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input \"layer\"\n",
        "class Layer_Input:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, training):\n",
        "        self.output = inputs"
      ],
      "metadata": {
        "id": "XLjTYM9H9Qcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss and Accuracy"
      ],
      "metadata": {
        "id": "cE2CVRUR9gSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Common loss class\n",
        "class Loss:\n",
        "\n",
        "    # Regularization loss calculation\n",
        "    def regularization_loss(self):\n",
        "\n",
        "        # 0 by default\n",
        "        regularization_loss = 0\n",
        "\n",
        "        # Calculate regularization loss\n",
        "        # iterate all trainable layers\n",
        "        for layer in self.trainable_layers:\n",
        "\n",
        "            # L1 regularization - weights\n",
        "            # calculate only when factor greater than 0\n",
        "            if layer.weight_regularizer_l1 > 0:\n",
        "                regularization_loss += layer.weight_regularizer_l1 * \\\n",
        "                                       np.sum(np.abs(layer.weights))\n",
        "\n",
        "            # L2 regularization - weights\n",
        "            if layer.weight_regularizer_l2 > 0:\n",
        "                regularization_loss += layer.weight_regularizer_l2 * \\\n",
        "                                       np.sum(layer.weights * \\\n",
        "                                              layer.weights)\n",
        "\n",
        "            # L1 regularization - biases\n",
        "            # calculate only when factor greater than 0\n",
        "            if layer.bias_regularizer_l1 > 0:\n",
        "                regularization_loss += layer.bias_regularizer_l1 * \\\n",
        "                                       np.sum(np.abs(layer.biases))\n",
        "\n",
        "            # L2 regularization - biases\n",
        "            if layer.bias_regularizer_l2 > 0:\n",
        "                regularization_loss += layer.bias_regularizer_l2 * \\\n",
        "                                       np.sum(layer.biases * \\\n",
        "                                              layer.biases)\n",
        "\n",
        "        return regularization_loss\n",
        "\n",
        "\n",
        "    # Set/remember trainable layers\n",
        "    def remember_trainable_layers(self, trainable_layers):\n",
        "        self.trainable_layers = trainable_layers\n",
        "\n",
        "    # Calculates the data and regularization losses\n",
        "    # given model output and ground truth values\n",
        "    def calculate(self, output, y, *, include_regularization=False):\n",
        "\n",
        "        # Calculate sample losses\n",
        "        sample_losses = self.forward(output, y)\n",
        "\n",
        "        # Calculate mean loss\n",
        "        data_loss = np.mean(sample_losses)\n",
        "\n",
        "        # Add accumulated sum of losses and sample count\n",
        "        self.accumulated_sum += np.sum(sample_losses)\n",
        "        self.accumulated_count += len(sample_losses)\n",
        "\n",
        "        # If just data loss - return it\n",
        "        if not include_regularization:\n",
        "            return data_loss\n",
        "\n",
        "        # Return the data and regularization losses\n",
        "        return data_loss, self.regularization_loss()\n",
        "\n",
        "    # Calculates accumulated loss\n",
        "    def calculate_accumulated(self, *, include_regularization=False):\n",
        "\n",
        "        # Calculate mean loss\n",
        "        data_loss = self.accumulated_sum / self.accumulated_count\n",
        "\n",
        "        # If just data loss - return it\n",
        "        if not include_regularization:\n",
        "            return data_loss\n",
        "\n",
        "        # Return the data and regularization losses\n",
        "        return data_loss, self.regularization_loss()\n",
        "\n",
        "    # Reset variables for accumulated loss\n",
        "    def new_pass(self):\n",
        "        self.accumulated_sum = 0\n",
        "        self.accumulated_count = 0"
      ],
      "metadata": {
        "id": "oiL2sd5p9fU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-entropy loss\n",
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "\n",
        "        # Number of samples in a batch\n",
        "        samples = len(y_pred)\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Probabilities for target values -\n",
        "        # only if categorical labels\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[\n",
        "                range(samples),\n",
        "                y_true\n",
        "            ]\n",
        "\n",
        "        # Mask values - only for one-hot encoded labels\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(\n",
        "                y_pred_clipped * y_true,\n",
        "                axis=1\n",
        "            )\n",
        "\n",
        "        # Losses\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "        return negative_log_likelihoods\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "        # Number of labels in every sample\n",
        "        # We'll use the first sample to count them\n",
        "        labels = len(dvalues[0])\n",
        "\n",
        "        # If labels are sparse, turn them into one-hot vector\n",
        "        if len(y_true.shape) == 1:\n",
        "            y_true = np.eye(labels)[y_true]\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dinputs = -y_true / dvalues\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples"
      ],
      "metadata": {
        "id": "79_E1vfW9exG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Common accuracy class\n",
        "class Accuracy:\n",
        "\n",
        "    # Calculates an accuracy\n",
        "    # given predictions and ground truth values\n",
        "    def calculate(self, predictions, y):\n",
        "\n",
        "        # Get comparison results\n",
        "        comparisons = self.compare(predictions, y)\n",
        "\n",
        "        # Calculate an accuracy\n",
        "        accuracy = np.mean(comparisons)\n",
        "\n",
        "        # Add accumulated sum of matching values and sample count\n",
        "        self.accumulated_sum += np.sum(comparisons)\n",
        "        self.accumulated_count += len(comparisons)\n",
        "\n",
        "        # Return accuracy\n",
        "        return accuracy\n",
        "\n",
        "    # Calculates accumulated accuracy\n",
        "    def calculate_accumulated(self):\n",
        "\n",
        "        # Calculate an accuracy\n",
        "        accuracy = self.accumulated_sum / self.accumulated_count\n",
        "\n",
        "        # Return the data and regularization losses\n",
        "        return accuracy\n",
        "\n",
        "    # Reset variables for accumulated accuracy\n",
        "    def new_pass(self):\n",
        "        self.accumulated_sum = 0\n",
        "        self.accumulated_count = 0"
      ],
      "metadata": {
        "id": "CNf1RE7_9rzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy calculation for classification model\n",
        "class Accuracy_Categorical(Accuracy):\n",
        "\n",
        "    def __init__(self, *, binary=False):\n",
        "        # Binary mode?\n",
        "        self.binary = binary\n",
        "\n",
        "    # No initialization is needed\n",
        "    def init(self, y):\n",
        "        pass\n",
        "\n",
        "    # Compares predictions to the ground truth values\n",
        "    def compare(self, predictions, y):\n",
        "        if not self.binary and len(y.shape) == 2:\n",
        "            y = np.argmax(y, axis=1)\n",
        "        return predictions == y"
      ],
      "metadata": {
        "id": "jMjIU5r29xmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer"
      ],
      "metadata": {
        "id": "Diqt95Vu8WCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD optimizer\n",
        "class Optimizer_SGD:\n",
        "\n",
        "    # Initialize optimizer - set settings,\n",
        "    # learning rate of 1. is default for this optimizer\n",
        "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        weight_updates = -self.current_learning_rate * \\\n",
        "                          layer.dweights\n",
        "        bias_updates = -self.current_learning_rate * \\\n",
        "                        layer.dbiases\n",
        "\n",
        "        # Update weights and biases using either\n",
        "        # vanilla or momentum updates\n",
        "        layer.weights += weight_updates\n",
        "        layer.biases += bias_updates\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n"
      ],
      "metadata": {
        "id": "xRS5GZl285Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training using Backpropagation"
      ],
      "metadata": {
        "id": "y_XeHHJM98P8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model class\n",
        "class Model:\n",
        "\n",
        "    def __init__(self):\n",
        "        # Create a list of network objects\n",
        "        self.layers = []\n",
        "        # Softmax classifier's output object\n",
        "        self.softmax_classifier_output = None\n",
        "\n",
        "    # Add objects to the model\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "\n",
        "    # Set loss, optimizer and accuracy\n",
        "    def set(self, *, loss, optimizer, accuracy):\n",
        "        self.loss = loss\n",
        "        self.optimizer = optimizer\n",
        "        self.accuracy = accuracy\n",
        "\n",
        "    # Finalize the model\n",
        "    def finalize(self):\n",
        "\n",
        "        # Create and set the input layer\n",
        "        self.input_layer = Layer_Input()\n",
        "\n",
        "        # Count all the objects\n",
        "        layer_count = len(self.layers)\n",
        "\n",
        "        # Initialize a list containing trainable layers:\n",
        "        self.trainable_layers = []\n",
        "\n",
        "        # Iterate the objects\n",
        "        for i in range(layer_count):\n",
        "\n",
        "            # If it's the first layer,\n",
        "            # the previous layer object is the input layer\n",
        "            if i == 0:\n",
        "                self.layers[i].prev = self.input_layer\n",
        "                self.layers[i].next = self.layers[i+1]\n",
        "\n",
        "            # All layers except for the first and the last\n",
        "            elif i < layer_count - 1:\n",
        "                self.layers[i].prev = self.layers[i-1]\n",
        "                self.layers[i].next = self.layers[i+1]\n",
        "\n",
        "            # The last layer - the next object is the loss\n",
        "            # Also let's save aside the reference to the last object\n",
        "            # whose output is the model's output\n",
        "            else:\n",
        "                self.layers[i].prev = self.layers[i-1]\n",
        "                self.layers[i].next = self.loss\n",
        "                self.output_layer_activation = self.layers[i]\n",
        "\n",
        "            # If layer contains an attribute called \"weights\",\n",
        "            # it's a trainable layer -\n",
        "            # add it to the list of trainable layers\n",
        "            # We don't need to check for biases -\n",
        "            # checking for weights is enough\n",
        "            if hasattr(self.layers[i], 'weights'):\n",
        "                self.trainable_layers.append(self.layers[i])\n",
        "\n",
        "\n",
        "        # Update loss object with trainable layers\n",
        "        self.loss.remember_trainable_layers(\n",
        "            self.trainable_layers\n",
        "        )\n",
        "\n",
        " \n",
        "\n",
        "    # Train the model\n",
        "    def train(self, X, y, *, epochs=1, batch_size=None,\n",
        "              print_every=1, validation_data=None):\n",
        "\n",
        "        # Initialize accuracy object\n",
        "        self.accuracy.init(y)\n",
        "\n",
        "        # Default value if batch size is not being set\n",
        "        train_steps = 1\n",
        "\n",
        "        # If there is validation data passed,\n",
        "        # set default number of steps for validation as well\n",
        "        if validation_data is not None:\n",
        "            validation_steps = 1\n",
        "\n",
        "            # For better readability\n",
        "            X_val, y_val = validation_data\n",
        "\n",
        "        # Calculate number of steps\n",
        "        if batch_size is not None:\n",
        "            train_steps = len(X) // batch_size\n",
        "            # Dividing rounds down. If there are some remaining\n",
        "            # data but not a full batch, this won't include it\n",
        "            # Add `1` to include this not full batch\n",
        "            if train_steps * batch_size < len(X):\n",
        "                train_steps += 1\n",
        "\n",
        "            if validation_data is not None:\n",
        "                validation_steps = len(X_val) // batch_size\n",
        "\n",
        "                # Dividing rounds down. If there are some remaining\n",
        "                # data but nor full batch, this won't include it\n",
        "                # Add `1` to include this not full batch\n",
        "                if validation_steps * batch_size < len(X_val):\n",
        "                    validation_steps += 1\n",
        "\n",
        "        # Main training loop\n",
        "        for epoch in range(1, epochs+1):\n",
        "\n",
        "            # Print epoch number\n",
        "            print('epoch: {} '.format(epoch))\n",
        "\n",
        "            # Reset accumulated values in loss and accuracy objects\n",
        "            self.loss.new_pass()\n",
        "            self.accuracy.new_pass()\n",
        "\n",
        "            # Iterate over steps\n",
        "            for step in range(train_steps):\n",
        "\n",
        "                # If batch size is not set -\n",
        "                # train using one step and full dataset\n",
        "                if batch_size is None:\n",
        "                    batch_X = X\n",
        "                    batch_y = y\n",
        "\n",
        "                # Otherwise slice a batch\n",
        "                else:\n",
        "                    batch_X = X[step*batch_size:(step+1)*batch_size]\n",
        "                    batch_y = y[step*batch_size:(step+1)*batch_size]\n",
        "\n",
        "                # Perform the forward pass\n",
        "                output = self.forward(batch_X, training=True)\n",
        "\n",
        "                # Calculate loss\n",
        "                data_loss, regularization_loss = \\\n",
        "                    self.loss.calculate(output, batch_y,\n",
        "                                        include_regularization=True)\n",
        "                loss = data_loss + regularization_loss\n",
        "\n",
        "                # Get predictions and calculate an accuracy\n",
        "                predictions = self.output_layer_activation.predictions(\n",
        "                                  output)\n",
        "                accuracy = self.accuracy.calculate(predictions,\n",
        "                                                   batch_y)\n",
        "\n",
        "                # Perform backward pass\n",
        "                self.backward(output, batch_y)\n",
        "\n",
        "\n",
        "                # Optimize (update parameters)\n",
        "                self.optimizer.pre_update_params()\n",
        "                for layer in self.trainable_layers:\n",
        "                    self.optimizer.update_params(layer)\n",
        "                self.optimizer.post_update_params()\n",
        "\n",
        "                # Print a summary\n",
        "                if not step % print_every or step == train_steps - 1:\n",
        "                    print(f'step: {step}, ' +\n",
        "                          f'acc: {accuracy:.3f}, ' +\n",
        "                          f'loss: {loss:.3f} (' +\n",
        "                          f'data_loss: {data_loss:.3f}, ' +\n",
        "                          f'reg_loss: {regularization_loss:.3f}), ' +\n",
        "                          f'lr: {self.optimizer.current_learning_rate}')\n",
        "\n",
        "            # Get and print epoch loss and accuracy\n",
        "            epoch_data_loss, epoch_regularization_loss = \\\n",
        "                self.loss.calculate_accumulated(\n",
        "                    include_regularization=True)\n",
        "            epoch_loss = epoch_data_loss + epoch_regularization_loss\n",
        "            epoch_accuracy = self.accuracy.calculate_accumulated()\n",
        "\n",
        "            print(f'training, ' +\n",
        "                  f'acc: {epoch_accuracy:.3f}, ' +\n",
        "                  f'loss: {epoch_loss:.3f} (' +\n",
        "                  f'data_loss: {epoch_data_loss:.3f}, ' +\n",
        "                  f'reg_loss: {epoch_regularization_loss:.3f}), ' +\n",
        "                  f'lr: {self.optimizer.current_learning_rate}')\n",
        "\n",
        "            # If there is the validation data\n",
        "            if validation_data is not None:\n",
        "\n",
        "                # Reset accumulated values in loss\n",
        "                # and accuracy objects\n",
        "                self.loss.new_pass()\n",
        "                self.accuracy.new_pass()\n",
        "\n",
        "                # Iterate over steps\n",
        "                for step in range(validation_steps):\n",
        "\n",
        "                    # If batch size is not set -\n",
        "                    # train using one step and full dataset\n",
        "                    if batch_size is None:\n",
        "                        batch_X = X_val\n",
        "                        batch_y = y_val\n",
        "\n",
        "\n",
        "                    # Otherwise slice a batch\n",
        "                    else:\n",
        "                        batch_X = X_val[\n",
        "                            step*batch_size:(step+1)*batch_size\n",
        "                        ]\n",
        "                        batch_y = y_val[\n",
        "                            step*batch_size:(step+1)*batch_size\n",
        "                        ]\n",
        "\n",
        "                    # Perform the forward pass\n",
        "                    output = self.forward(batch_X, training=False)\n",
        "\n",
        "                    # Calculate the loss\n",
        "                    self.loss.calculate(output, batch_y)\n",
        "\n",
        "                    # Get predictions and calculate an accuracy\n",
        "                    predictions = self.output_layer_activation.predictions(\n",
        "                                      output)\n",
        "                    self.accuracy.calculate(predictions, batch_y)\n",
        "\n",
        "                # Get and print validation loss and accuracy\n",
        "                validation_loss = self.loss.calculate_accumulated()\n",
        "                validation_accuracy = self.accuracy.calculate_accumulated()\n",
        "\n",
        "                # Print a summary\n",
        "                print(f'validation, ' +\n",
        "                      f'acc: {validation_accuracy:.3f}, ' +\n",
        "                      f'loss: {validation_loss:.3f}')\n",
        "\n",
        "    # Performs forward pass\n",
        "    def forward(self, X, training):\n",
        "\n",
        "        # Call forward method on the input layer\n",
        "        # this will set the output property that\n",
        "        # the first layer in \"prev\" object is expecting\n",
        "        self.input_layer.forward(X, training)\n",
        "\n",
        "        # Call forward method of every object in a chain\n",
        "        # Pass output of the previous object as a parameter\n",
        "        for layer in self.layers:\n",
        "            layer.forward(layer.prev.output, training)\n",
        "\n",
        "        # \"layer\" is now the last object from the list,\n",
        "        # return its output\n",
        "        return layer.output\n",
        "\n",
        "\n",
        "    # Performs backward pass\n",
        "    def backward(self, output, y):\n",
        "\n",
        "        # If softmax classifier\n",
        "        if self.softmax_classifier_output is not None:\n",
        "            # First call backward method\n",
        "            # on the combined activation/loss\n",
        "            # this will set dinputs property\n",
        "            self.softmax_classifier_output.backward(output, y)\n",
        "\n",
        "            # Since we'll not call backward method of the last layer\n",
        "            # which is Softmax activation\n",
        "            # as we used combined activation/loss\n",
        "            # object, let's set dinputs in this object\n",
        "            self.layers[-1].dinputs = \\\n",
        "                self.softmax_classifier_output.dinputs\n",
        "\n",
        "            # Call backward method going through\n",
        "            # all the objects but last\n",
        "            # in reversed order passing dinputs as a parameter\n",
        "            for layer in reversed(self.layers[:-1]):\n",
        "                layer.backward(layer.next.dinputs)\n",
        "\n",
        "            return\n",
        "\n",
        "        # First call backward method on the loss\n",
        "        # this will set dinputs property that the last\n",
        "        # layer will try to access shortly\n",
        "        self.loss.backward(output, y)\n",
        "\n",
        "        # Call backward method going through all the objects\n",
        "        # in reversed order passing dinputs as a parameter\n",
        "        for layer in reversed(self.layers):\n",
        "            layer.backward(layer.next.dinputs)\n"
      ],
      "metadata": {
        "id": "lOJCtINq97Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "QgFhXBpy8dML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = Model()\n",
        "\n",
        "\n",
        "# Add layers\n",
        "model.add(Layer_Dense(X.shape[1], 128))\n",
        "model.add(Activation_ReLU())\n",
        "model.add(Layer_Dense(128, 64))\n",
        "model.add(Activation_ReLU())\n",
        "model.add(Layer_Dense(64, 10))\n",
        "model.add(Activation_Softmax())\n",
        "\n",
        "# Set loss, optimizer and accuracy objects\n",
        "model.set(\n",
        "    loss=Loss_CategoricalCrossentropy(),\n",
        "    optimizer=Optimizer_SGD(decay=1e-3),\n",
        "    accuracy=Accuracy_Categorical()\n",
        ")\n",
        "\n",
        "# Finalize the model\n",
        "model.finalize()\n",
        "\n",
        "# Train the model\n",
        "model.train(X, y, validation_data=(X_test, y_test),\n",
        "            epochs=10, batch_size=128, print_every=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-kEeJGd8LNU",
        "outputId": "22301429-1c2f-48ca-d407-0c5a61bb7181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 \n",
            "step: 0, acc: 0.055, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 1.0\n",
            "step: 100, acc: 0.367, loss: 2.000 (data_loss: 2.000, reg_loss: 0.000), lr: 0.9090909090909091\n",
            "step: 200, acc: 0.125, loss: 3.125 (data_loss: 3.125, reg_loss: 0.000), lr: 0.8333333333333334\n",
            "step: 300, acc: 0.445, loss: 1.444 (data_loss: 1.444, reg_loss: 0.000), lr: 0.7692307692307692\n",
            "step: 400, acc: 0.375, loss: 1.331 (data_loss: 1.331, reg_loss: 0.000), lr: 0.7142857142857143\n",
            "step: 468, acc: 0.469, loss: 1.133 (data_loss: 1.133, reg_loss: 0.000), lr: 0.6811989100817439\n",
            "training, acc: 0.300, loss: 1.818 (data_loss: 1.818, reg_loss: 0.000), lr: 0.6811989100817439\n",
            "validation, acc: 0.485, loss: 1.054\n",
            "epoch: 2 \n",
            "step: 0, acc: 0.516, loss: 0.974 (data_loss: 0.974, reg_loss: 0.000), lr: 0.6807351940095303\n",
            "step: 100, acc: 0.367, loss: 1.304 (data_loss: 1.304, reg_loss: 0.000), lr: 0.6373486297004461\n",
            "step: 200, acc: 0.398, loss: 1.254 (data_loss: 1.254, reg_loss: 0.000), lr: 0.5991611743559018\n",
            "step: 300, acc: 0.531, loss: 1.173 (data_loss: 1.173, reg_loss: 0.000), lr: 0.5652911249293385\n",
            "step: 400, acc: 0.469, loss: 1.048 (data_loss: 1.048, reg_loss: 0.000), lr: 0.5350454788657036\n",
            "step: 468, acc: 0.521, loss: 1.063 (data_loss: 1.063, reg_loss: 0.000), lr: 0.5162622612287042\n",
            "training, acc: 0.436, loss: 1.295 (data_loss: 1.295, reg_loss: 0.000), lr: 0.5162622612287042\n",
            "validation, acc: 0.485, loss: 1.126\n",
            "epoch: 3 \n",
            "step: 0, acc: 0.539, loss: 1.055 (data_loss: 1.055, reg_loss: 0.000), lr: 0.5159958720330237\n",
            "step: 100, acc: 0.641, loss: 0.811 (data_loss: 0.811, reg_loss: 0.000), lr: 0.4906771344455348\n",
            "step: 200, acc: 0.656, loss: 0.763 (data_loss: 0.763, reg_loss: 0.000), lr: 0.4677268475210477\n",
            "step: 300, acc: 0.484, loss: 0.947 (data_loss: 0.947, reg_loss: 0.000), lr: 0.44682752457551383\n",
            "step: 400, acc: 0.648, loss: 0.684 (data_loss: 0.684, reg_loss: 0.000), lr: 0.42771599657827203\n",
            "step: 468, acc: 0.615, loss: 0.841 (data_loss: 0.841, reg_loss: 0.000), lr: 0.4156275976724854\n",
            "training, acc: 0.595, loss: 0.880 (data_loss: 0.880, reg_loss: 0.000), lr: 0.4156275976724854\n",
            "validation, acc: 0.601, loss: 0.878\n",
            "epoch: 4 \n",
            "step: 0, acc: 0.688, loss: 0.643 (data_loss: 0.643, reg_loss: 0.000), lr: 0.4154549231408392\n",
            "step: 100, acc: 0.648, loss: 0.748 (data_loss: 0.748, reg_loss: 0.000), lr: 0.39888312724371755\n",
            "step: 200, acc: 0.719, loss: 0.737 (data_loss: 0.737, reg_loss: 0.000), lr: 0.3835826620636747\n",
            "step: 300, acc: 0.578, loss: 0.876 (data_loss: 0.876, reg_loss: 0.000), lr: 0.3694126339120798\n",
            "step: 400, acc: 0.703, loss: 0.680 (data_loss: 0.680, reg_loss: 0.000), lr: 0.3562522265764161\n",
            "step: 468, acc: 0.781, loss: 0.553 (data_loss: 0.553, reg_loss: 0.000), lr: 0.34782608695652173\n",
            "training, acc: 0.669, loss: 0.768 (data_loss: 0.768, reg_loss: 0.000), lr: 0.34782608695652173\n",
            "validation, acc: 0.695, loss: 0.809\n",
            "epoch: 5 \n",
            "step: 0, acc: 0.750, loss: 0.595 (data_loss: 0.595, reg_loss: 0.000), lr: 0.3477051460361613\n",
            "step: 100, acc: 0.805, loss: 0.540 (data_loss: 0.540, reg_loss: 0.000), lr: 0.33602150537634407\n",
            "step: 200, acc: 0.758, loss: 0.587 (data_loss: 0.587, reg_loss: 0.000), lr: 0.3250975292587776\n",
            "step: 300, acc: 0.641, loss: 0.894 (data_loss: 0.894, reg_loss: 0.000), lr: 0.3148614609571788\n",
            "step: 400, acc: 0.836, loss: 0.396 (data_loss: 0.396, reg_loss: 0.000), lr: 0.3052503052503052\n",
            "step: 468, acc: 0.729, loss: 0.603 (data_loss: 0.603, reg_loss: 0.000), lr: 0.29904306220095694\n",
            "training, acc: 0.766, loss: 0.629 (data_loss: 0.629, reg_loss: 0.000), lr: 0.29904306220095694\n",
            "validation, acc: 0.766, loss: 0.690\n",
            "epoch: 6 \n",
            "step: 0, acc: 0.844, loss: 0.520 (data_loss: 0.520, reg_loss: 0.000), lr: 0.2989536621823617\n",
            "step: 100, acc: 0.852, loss: 0.438 (data_loss: 0.438, reg_loss: 0.000), lr: 0.29027576197387517\n",
            "step: 200, acc: 0.836, loss: 0.511 (data_loss: 0.511, reg_loss: 0.000), lr: 0.2820874471086037\n",
            "step: 300, acc: 0.859, loss: 0.436 (data_loss: 0.436, reg_loss: 0.000), lr: 0.27434842249657065\n",
            "step: 400, acc: 0.891, loss: 0.334 (data_loss: 0.334, reg_loss: 0.000), lr: 0.26702269692923897\n",
            "step: 468, acc: 0.812, loss: 0.474 (data_loss: 0.474, reg_loss: 0.000), lr: 0.26226068712300027\n",
            "training, acc: 0.826, loss: 0.491 (data_loss: 0.491, reg_loss: 0.000), lr: 0.26226068712300027\n",
            "validation, acc: 0.810, loss: 0.530\n",
            "epoch: 7 \n",
            "step: 0, acc: 0.875, loss: 0.328 (data_loss: 0.328, reg_loss: 0.000), lr: 0.26219192448872575\n",
            "step: 100, acc: 0.859, loss: 0.409 (data_loss: 0.409, reg_loss: 0.000), lr: 0.25549310168625444\n",
            "step: 200, acc: 0.844, loss: 0.478 (data_loss: 0.478, reg_loss: 0.000), lr: 0.24912805181863476\n",
            "step: 300, acc: 0.852, loss: 0.406 (data_loss: 0.406, reg_loss: 0.000), lr: 0.24307243558580457\n",
            "step: 400, acc: 0.906, loss: 0.303 (data_loss: 0.303, reg_loss: 0.000), lr: 0.23730422401518744\n",
            "step: 468, acc: 0.844, loss: 0.395 (data_loss: 0.395, reg_loss: 0.000), lr: 0.2335357309668379\n",
            "training, acc: 0.841, loss: 0.445 (data_loss: 0.445, reg_loss: 0.000), lr: 0.2335357309668379\n",
            "validation, acc: 0.816, loss: 0.513\n",
            "epoch: 8 \n",
            "step: 0, acc: 0.883, loss: 0.298 (data_loss: 0.298, reg_loss: 0.000), lr: 0.2334812047630166\n",
            "step: 100, acc: 0.875, loss: 0.384 (data_loss: 0.384, reg_loss: 0.000), lr: 0.22815423226100845\n",
            "step: 200, acc: 0.844, loss: 0.461 (data_loss: 0.461, reg_loss: 0.000), lr: 0.22306491188935978\n",
            "step: 300, acc: 0.859, loss: 0.389 (data_loss: 0.389, reg_loss: 0.000), lr: 0.21819768710451667\n",
            "step: 400, acc: 0.898, loss: 0.293 (data_loss: 0.293, reg_loss: 0.000), lr: 0.21353833013025839\n",
            "step: 468, acc: 0.875, loss: 0.377 (data_loss: 0.377, reg_loss: 0.000), lr: 0.2104820037886761\n",
            "training, acc: 0.851, loss: 0.417 (data_loss: 0.417, reg_loss: 0.000), lr: 0.2104820037886761\n",
            "validation, acc: 0.823, loss: 0.499\n",
            "epoch: 9 \n",
            "step: 0, acc: 0.898, loss: 0.290 (data_loss: 0.290, reg_loss: 0.000), lr: 0.2104377104377104\n",
            "step: 100, acc: 0.859, loss: 0.389 (data_loss: 0.389, reg_loss: 0.000), lr: 0.2061005770816158\n",
            "step: 200, acc: 0.852, loss: 0.437 (data_loss: 0.437, reg_loss: 0.000), lr: 0.20193861066235866\n",
            "step: 300, acc: 0.852, loss: 0.388 (data_loss: 0.388, reg_loss: 0.000), lr: 0.1979414093428345\n",
            "step: 400, acc: 0.891, loss: 0.274 (data_loss: 0.274, reg_loss: 0.000), lr: 0.19409937888198758\n",
            "step: 468, acc: 0.865, loss: 0.363 (data_loss: 0.363, reg_loss: 0.000), lr: 0.19157088122605365\n",
            "training, acc: 0.857, loss: 0.399 (data_loss: 0.399, reg_loss: 0.000), lr: 0.19157088122605365\n",
            "validation, acc: 0.823, loss: 0.498\n",
            "epoch: 10 \n",
            "step: 0, acc: 0.891, loss: 0.301 (data_loss: 0.301, reg_loss: 0.000), lr: 0.1915341888527102\n",
            "step: 100, acc: 0.867, loss: 0.369 (data_loss: 0.369, reg_loss: 0.000), lr: 0.18793459875963167\n",
            "step: 200, acc: 0.867, loss: 0.436 (data_loss: 0.436, reg_loss: 0.000), lr: 0.18446781036709092\n",
            "step: 300, acc: 0.859, loss: 0.389 (data_loss: 0.389, reg_loss: 0.000), lr: 0.18112660749864154\n",
            "step: 400, acc: 0.891, loss: 0.252 (data_loss: 0.252, reg_loss: 0.000), lr: 0.17790428749332857\n",
            "step: 468, acc: 0.865, loss: 0.332 (data_loss: 0.332, reg_loss: 0.000), lr: 0.17577781683951485\n",
            "training, acc: 0.861, loss: 0.384 (data_loss: 0.384, reg_loss: 0.000), lr: 0.17577781683951485\n",
            "validation, acc: 0.829, loss: 0.483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, I built a dense neural network model to classify the MNIST Fashion dataset using no library except Numpy and predicted the accuracy at around 82%.\n",
        "\n",
        "It was an uphill task to do this using plain numpy. So a lot of contribution goes to the following resources.\n",
        "- Samson Zhang's [channel](https://www.youtube.com/watch?v=w8yWXqWQYmU&t=8s)\n",
        "- I wanted to use Adagrad or Adam but found them hard to work with plain numpy. And this lead me to the wonderful tutorial of Harrison Kinsely. I worked with SGD, and took help from his code (specially the transition to Objective Oriented Programming), as well as the depth and the architecture. However, I have done some edits myself to meet my needs.\n"
      ],
      "metadata": {
        "id": "AgqGbqaD5EaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bq5VOamX_cqN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}